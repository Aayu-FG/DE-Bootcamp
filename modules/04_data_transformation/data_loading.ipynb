{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Transformation\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Load data from various sources using pandas\n",
    "- Transform data during the loading process\n",
    "- Handle different data formats and encodings\n",
    "- Combine data from multiple sources\n",
    "- Optimize data loading for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data from Different Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime, timedelta\n",
    "import io\n",
    "\n",
    "# Create sample data files for demonstration\n",
    "def create_sample_data_files() -> None:\n",
    "    \"\"\"\n",
    "    Create sample data files in different formats for loading examples.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. CSV file with sales data\n",
    "    sales_data = {\n",
    "        'transaction_id': [f'TXN{i:04d}' for i in range(1, 101)],\n",
    "        'customer_id': [f'CUST{np.random.randint(1, 21):03d}' for _ in range(100)],\n",
    "        'product_name': np.random.choice(['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam'], 100),\n",
    "        'category': np.random.choice(['Electronics', 'Accessories'], 100),\n",
    "        'quantity': np.random.randint(1, 5, 100),\n",
    "        'unit_price': np.round(np.random.uniform(10, 500, 100), 2),\n",
    "        'discount_percent': np.round(np.random.uniform(0, 20, 100), 1),\n",
    "        'sale_date': pd.date_range('2024-01-01', periods=100, freq='D')[:100].strftime('%Y-%m-%d'),\n",
    "        'sales_rep': np.random.choice(['Alice', 'Bob', 'Charlie', 'Diana'], 100)\n",
    "    }\n",
    "    \n",
    "    df_sales = pd.DataFrame(sales_data)\n",
    "    df_sales.to_csv('sales_data.csv', index=False)\n",
    "    \n",
    "    # 2. JSON file with customer data\n",
    "    customers = []\n",
    "    for i in range(1, 21):\n",
    "        customer = {\n",
    "            'customer_id': f'CUST{i:03d}',\n",
    "            'name': f'Customer {i}',\n",
    "            'email': f'customer{i}@example.com',\n",
    "            'registration_date': (datetime(2023, 1, 1) + timedelta(days=i*5)).strftime('%Y-%m-%d'),\n",
    "            'address': {\n",
    "                'street': f'{100 + i} Main St',\n",
    "                'city': np.random.choice(['New York', 'London', 'Tokyo', 'Sydney']),\n",
    "                'country': 'USA'\n",
    "            },\n",
    "            'preferences': {\n",
    "                'newsletter': np.random.choice([True, False]),\n",
    "                'preferred_category': np.random.choice(['Electronics', 'Accessories'])\n",
    "            }\n",
    "        }\n",
    "        customers.append(customer)\n",
    "    \n",
    "    with open('customers.json', 'w') as f:\n",
    "        json.dump({'customers': customers}, f, indent=2)\n",
    "    \n",
    "    # 3. Tab-separated file with product data\n",
    "    products_data = {\n",
    "        'product_id': [f'PRD{i:03d}' for i in range(1, 11)],\n",
    "        'product_name': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Tablet', 'Phone', 'Headphones', 'Speaker', 'Camera'],\n",
    "        'category': ['Electronics'] * 10,\n",
    "        'cost_price': [500, 15, 40, 200, 60, 300, 400, 80, 120, 250],\n",
    "        'retail_price': [999, 29, 79, 399, 89, 599, 799, 149, 199, 499],\n",
    "        'supplier': ['TechCorp', 'AccessoryCo', 'AccessoryCo', 'DisplayTech', 'AccessoryCo', 'TechCorp', 'MobileTech', 'AudioTech', 'AudioTech', 'PhotoTech']\n",
    "    }\n",
    "    \n",
    "    df_products = pd.DataFrame(products_data)\n",
    "    df_products.to_csv('products.tsv', sep='\\t', index=False)\n",
    "    \n",
    "    print(\"Sample data files created:\")\n",
    "    print(\"✓ sales_data.csv (100 sales transactions)\")\n",
    "    print(\"✓ customers.json (20 customer records)\")\n",
    "    print(\"✓ products.tsv (10 product records)\")\n",
    "\n",
    "# Create sample files\n",
    "create_sample_data_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading CSV files with different options\n",
    "print(\"=== Loading CSV Files ===\")\n",
    "\n",
    "# Basic CSV loading\n",
    "df_sales_basic: pd.DataFrame = pd.read_csv('sales_data.csv')\n",
    "print(f\"Basic CSV load: {df_sales_basic.shape}\")\n",
    "print(f\"Data types:\\n{df_sales_basic.dtypes}\")\n",
    "\n",
    "# CSV loading with data type specification\n",
    "dtype_spec = {\n",
    "    'transaction_id': 'string',\n",
    "    'customer_id': 'string',\n",
    "    'product_name': 'category',\n",
    "    'category': 'category',\n",
    "    'quantity': 'int32',\n",
    "    'unit_price': 'float64',\n",
    "    'discount_percent': 'float32',\n",
    "    'sales_rep': 'category'\n",
    "}\n",
    "\n",
    "df_sales_typed: pd.DataFrame = pd.read_csv(\n",
    "    'sales_data.csv',\n",
    "    dtype=dtype_spec,\n",
    "    parse_dates=['sale_date']\n",
    ")\n",
    "\n",
    "print(f\"\\nOptimized CSV load with types:\")\n",
    "print(f\"Data types:\\n{df_sales_typed.dtypes}\")\n",
    "print(f\"Memory usage comparison:\")\n",
    "print(f\"Basic: {df_sales_basic.memory_usage(deep=True).sum()} bytes\")\n",
    "print(f\"Typed: {df_sales_typed.memory_usage(deep=True).sum()} bytes\")\n",
    "\n",
    "# Loading with column selection\n",
    "selected_columns = ['transaction_id', 'customer_id', 'product_name', 'unit_price', 'sale_date']\n",
    "df_sales_subset: pd.DataFrame = pd.read_csv(\n",
    "    'sales_data.csv',\n",
    "    usecols=selected_columns,\n",
    "    parse_dates=['sale_date']\n",
    ")\n",
    "\n",
    "print(f\"\\nSubset loading: {df_sales_subset.shape}\")\n",
    "print(df_sales_subset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading JSON files\n",
    "print(\"=== Loading JSON Files ===\")\n",
    "\n",
    "# Load JSON file\n",
    "with open('customers.json', 'r') as f:\n",
    "    customers_json = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_customers_raw: pd.DataFrame = pd.DataFrame(customers_json['customers'])\n",
    "print(f\"Raw JSON DataFrame: {df_customers_raw.shape}\")\n",
    "print(f\"Columns: {df_customers_raw.columns.tolist()}\")\n",
    "print(df_customers_raw.head(2))\n",
    "\n",
    "# Normalize nested JSON data\n",
    "df_customers_normalized: pd.DataFrame = pd.json_normalize(customers_json['customers'])\n",
    "print(f\"\\nNormalized JSON DataFrame: {df_customers_normalized.shape}\")\n",
    "print(f\"Columns: {df_customers_normalized.columns.tolist()}\")\n",
    "print(df_customers_normalized.head(2))\n",
    "\n",
    "# Load JSON directly with pandas (for simple structures)\n",
    "try:\n",
    "    df_customers_direct: pd.DataFrame = pd.read_json('customers.json')\n",
    "    print(f\"\\nDirect JSON load: {df_customers_direct.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nDirect JSON load failed: {e}\")\n",
    "    print(\"This is expected for nested JSON structures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading tab-separated and other delimited files\n",
    "print(\"=== Loading Delimited Files ===\")\n",
    "\n",
    "# Load TSV file\n",
    "df_products: pd.DataFrame = pd.read_csv('products.tsv', sep='\\t')\n",
    "print(f\"TSV file loaded: {df_products.shape}\")\n",
    "print(df_products.head())\n",
    "\n",
    "# Loading with custom parameters\n",
    "df_products_custom: pd.DataFrame = pd.read_csv(\n",
    "    'products.tsv',\n",
    "    sep='\\t',\n",
    "    dtype={\n",
    "        'product_id': 'string',\n",
    "        'product_name': 'string',\n",
    "        'category': 'category',\n",
    "        'supplier': 'category'\n",
    "    },\n",
    "    index_col='product_id'\n",
    ")\n",
    "\n",
    "print(f\"\\nCustom TSV load with index:\")\n",
    "print(df_products_custom.head())\n",
    "print(f\"Index: {df_products_custom.index.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Transformation During Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data during loading\n",
    "print(\"=== Data Transformation During Loading ===\")\n",
    "\n",
    "# Custom date parser\n",
    "def custom_date_parser(date_str: str) -> pd.Timestamp:\n",
    "    \"\"\"\n",
    "    Custom date parser for specific date formats.\n",
    "    \"\"\"\n",
    "    return pd.to_datetime(date_str, format='%Y-%m-%d')\n",
    "\n",
    "# Load with transformations\n",
    "df_sales_transformed: pd.DataFrame = pd.read_csv(\n",
    "    'sales_data.csv',\n",
    "    dtype={\n",
    "        'transaction_id': 'string',\n",
    "        'customer_id': 'string',\n",
    "        'product_name': 'category',\n",
    "        'category': 'category',\n",
    "        'sales_rep': 'category'\n",
    "    },\n",
    "    parse_dates=['sale_date'],\n",
    "    date_parser=custom_date_parser\n",
    ")\n",
    "\n",
    "# Add calculated columns during loading process\n",
    "df_sales_transformed['total_amount'] = (\n",
    "    df_sales_transformed['quantity'] * df_sales_transformed['unit_price']\n",
    ")\n",
    "df_sales_transformed['discount_amount'] = (\n",
    "    df_sales_transformed['total_amount'] * df_sales_transformed['discount_percent'] / 100\n",
    ")\n",
    "df_sales_transformed['final_amount'] = (\n",
    "    df_sales_transformed['total_amount'] - df_sales_transformed['discount_amount']\n",
    ")\n",
    "\n",
    "print(f\"Transformed sales data: {df_sales_transformed.shape}\")\n",
    "print(df_sales_transformed[['transaction_id', 'total_amount', 'discount_amount', 'final_amount']].head())\n",
    "\n",
    "# Custom converters for specific columns\n",
    "def price_converter(price_str: str) -> float:\n",
    "    \"\"\"\n",
    "    Convert price string to float, handling currency symbols.\n",
    "    \"\"\"\n",
    "    if isinstance(price_str, str):\n",
    "        return float(price_str.replace('$', '').replace(',', ''))\n",
    "    return float(price_str)\n",
    "\n",
    "# Example with converters (creating sample data with currency symbols)\n",
    "sample_data_with_currency = \"\"\"\n",
    "product,price,quantity\n",
    "Laptop,\"$1,299.99\",1\n",
    "Mouse,$29.99,2\n",
    "Keyboard,$79.99,1\n",
    "\"\"\"\n",
    "\n",
    "df_currency: pd.DataFrame = pd.read_csv(\n",
    "    io.StringIO(sample_data_with_currency),\n",
    "    converters={'price': price_converter}\n",
    ")\n",
    "\n",
    "print(f\"\\nData with currency conversion:\")\n",
    "print(df_currency)\n",
    "print(f\"Price column type: {df_currency['price'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data in chunks for large files\n",
    "print(\"=== Chunked Data Loading ===\")\n",
    "\n",
    "# Simulate processing large file in chunks\n",
    "chunk_size = 25\n",
    "processed_chunks = []\n",
    "\n",
    "print(f\"Processing sales data in chunks of {chunk_size} rows:\")\n",
    "\n",
    "for chunk_num, chunk in enumerate(pd.read_csv('sales_data.csv', chunksize=chunk_size), 1):\n",
    "    # Process each chunk\n",
    "    chunk['total_amount'] = chunk['quantity'] * chunk['unit_price']\n",
    "    chunk['chunk_number'] = chunk_num\n",
    "    \n",
    "    # Calculate chunk statistics\n",
    "    chunk_stats = {\n",
    "        'chunk_number': chunk_num,\n",
    "        'rows': len(chunk),\n",
    "        'total_sales': chunk['total_amount'].sum(),\n",
    "        'avg_transaction': chunk['total_amount'].mean()\n",
    "    }\n",
    "    \n",
    "    processed_chunks.append(chunk_stats)\n",
    "    print(f\"  Chunk {chunk_num}: {len(chunk)} rows, Total: ${chunk['total_amount'].sum():.2f}\")\n",
    "\n",
    "# Combine chunk statistics\n",
    "df_chunk_stats: pd.DataFrame = pd.DataFrame(processed_chunks)\n",
    "print(f\"\\nChunk processing summary:\")\n",
    "print(df_chunk_stats)\n",
    "print(f\"Total across all chunks: ${df_chunk_stats['total_sales'].sum():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combining Data from Multiple Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining data from different sources\n",
    "print(\"=== Combining Multiple Data Sources ===\")\n",
    "\n",
    "# Load all data sources\n",
    "df_sales = pd.read_csv('sales_data.csv', parse_dates=['sale_date'])\n",
    "df_customers = pd.json_normalize(json.load(open('customers.json'))['customers'])\n",
    "df_products = pd.read_csv('products.tsv', sep='\\t')\n",
    "\n",
    "print(f\"Loaded data:\")\n",
    "print(f\"  Sales: {df_sales.shape}\")\n",
    "print(f\"  Customers: {df_customers.shape}\")\n",
    "print(f\"  Products: {df_products.shape}\")\n",
    "\n",
    "# Merge sales with customer data\n",
    "df_sales_customers: pd.DataFrame = pd.merge(\n",
    "    df_sales,\n",
    "    df_customers[['customer_id', 'name', 'address.city', 'preferences.preferred_category']],\n",
    "    on='customer_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\nSales + Customers: {df_sales_customers.shape}\")\n",
    "print(df_sales_customers[['transaction_id', 'customer_id', 'name', 'address.city']].head())\n",
    "\n",
    "# Merge with product data\n",
    "df_complete: pd.DataFrame = pd.merge(\n",
    "    df_sales_customers,\n",
    "    df_products[['product_name', 'cost_price', 'retail_price', 'supplier']],\n",
    "    on='product_name',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\nComplete dataset: {df_complete.shape}\")\n",
    "print(f\"Columns: {df_complete.columns.tolist()}\")\n",
    "\n",
    "# Calculate additional metrics with combined data\n",
    "df_complete['profit_per_unit'] = df_complete['unit_price'] - df_complete['cost_price']\n",
    "df_complete['total_profit'] = df_complete['profit_per_unit'] * df_complete['quantity']\n",
    "df_complete['margin_percent'] = (df_complete['profit_per_unit'] / df_complete['unit_price']) * 100\n",
    "\n",
    "print(f\"\\nSample of complete dataset with calculations:\")\n",
    "sample_cols = ['transaction_id', 'name', 'product_name', 'unit_price', 'cost_price', 'profit_per_unit', 'margin_percent']\n",
    "print(df_complete[sample_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different types of joins\n",
    "print(\"=== Different Join Types ===\")\n",
    "\n",
    "# Create sample datasets to demonstrate joins\n",
    "df_orders = pd.DataFrame({\n",
    "    'order_id': ['ORD001', 'ORD002', 'ORD003', 'ORD004'],\n",
    "    'customer_id': ['CUST001', 'CUST002', 'CUST003', 'CUST999'],  # CUST999 doesn't exist in customers\n",
    "    'order_amount': [150.00, 75.50, 200.00, 99.99]\n",
    "})\n",
    "\n",
    "df_customers_sample = df_customers[['customer_id', 'name', 'address.city']].head(3)\n",
    "\n",
    "print(f\"Orders data:\")\n",
    "print(df_orders)\n",
    "print(f\"\\nCustomers data:\")\n",
    "print(df_customers_sample)\n",
    "\n",
    "# Inner join (only matching records)\n",
    "inner_join: pd.DataFrame = pd.merge(df_orders, df_customers_sample, on='customer_id', how='inner')\n",
    "print(f\"\\nInner join: {inner_join.shape}\")\n",
    "print(inner_join)\n",
    "\n",
    "# Left join (all orders, matching customers)\n",
    "left_join: pd.DataFrame = pd.merge(df_orders, df_customers_sample, on='customer_id', how='left')\n",
    "print(f\"\\nLeft join: {left_join.shape}\")\n",
    "print(left_join)\n",
    "\n",
    "# Right join (all customers, matching orders)\n",
    "right_join: pd.DataFrame = pd.merge(df_orders, df_customers_sample, on='customer_id', how='right')\n",
    "print(f\"\\nRight join: {right_join.shape}\")\n",
    "print(right_join)\n",
    "\n",
    "# Outer join (all records from both)\n",
    "outer_join: pd.DataFrame = pd.merge(df_orders, df_customers_sample, on='customer_id', how='outer')\n",
    "print(f\"\\nOuter join: {outer_join.shape}\")\n",
    "print(outer_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading optimization techniques\n",
    "print(\"=== Data Loading Optimization ===\")\n",
    "\n",
    "import time\n",
    "\n",
    "# Create a larger sample file for performance testing\n",
    "def create_large_sample_file(filename: str, num_rows: int = 10000) -> None:\n",
    "    \"\"\"\n",
    "    Create a larger sample file for performance testing.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    large_data = {\n",
    "        'id': range(1, num_rows + 1),\n",
    "        'name': [f'Customer_{i}' for i in range(1, num_rows + 1)],\n",
    "        'category': np.random.choice(['A', 'B', 'C', 'D'], num_rows),\n",
    "        'value1': np.random.randn(num_rows),\n",
    "        'value2': np.random.randn(num_rows),\n",
    "        'value3': np.random.randn(num_rows),\n",
    "        'date': pd.date_range('2023-01-01', periods=num_rows, freq='H').strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    df_large = pd.DataFrame(large_data)\n",
    "    df_large.to_csv(filename, index=False)\n",
    "    print(f\"Created {filename} with {num_rows:,} rows\")\n",
    "\n",
    "create_large_sample_file('large_sample.csv', 10000)\n",
    "\n",
    "# Compare different loading strategies\n",
    "def time_loading_strategy(strategy_name: str, load_function) -> float:\n",
    "    \"\"\"\n",
    "    Time a loading strategy and return execution time.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    df = load_function()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    execution_time = end_time - start_time\n",
    "    memory_usage = df.memory_usage(deep=True).sum()\n",
    "    \n",
    "    print(f\"{strategy_name}:\")\n",
    "    print(f\"  Time: {execution_time:.4f} seconds\")\n",
    "    print(f\"  Memory: {memory_usage:,} bytes\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    \n",
    "    return execution_time\n",
    "\n",
    "# Strategy 1: Basic loading\n",
    "def load_basic():\n",
    "    return pd.read_csv('large_sample.csv')\n",
    "\n",
    "# Strategy 2: Optimized data types\n",
    "def load_optimized():\n",
    "    return pd.read_csv(\n",
    "        'large_sample.csv',\n",
    "        dtype={\n",
    "            'id': 'int32',\n",
    "            'name': 'string',\n",
    "            'category': 'category',\n",
    "            'value1': 'float32',\n",
    "            'value2': 'float32',\n",
    "            'value3': 'float32'\n",
    "        },\n",
    "        parse_dates=['date']\n",
    "    )\n",
    "\n",
    "# Strategy 3: Column selection\n",
    "def load_selected_columns():\n",
    "    return pd.read_csv(\n",
    "        'large_sample.csv',\n",
    "        usecols=['id', 'category', 'value1', 'date'],\n",
    "        dtype={\n",
    "            'id': 'int32',\n",
    "            'category': 'category',\n",
    "            'value1': 'float32'\n",
    "        },\n",
    "        parse_dates=['date']\n",
    "    )\n",
    "\n",
    "print(\"\\nPerformance comparison:\")\n",
    "time1 = time_loading_strategy(\"Basic loading\", load_basic)\n",
    "print()\n",
    "time2 = time_loading_strategy(\"Optimized types\", load_optimized)\n",
    "print()\n",
    "time3 = time_loading_strategy(\"Selected columns\", load_selected_columns)\n",
    "\n",
    "print(f\"\\nPerformance summary:\")\n",
    "print(f\"Optimized types is {time1/time2:.2f}x faster than basic\")\n",
    "print(f\"Selected columns is {time1/time3:.2f}x faster than basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data with error handling\n",
    "print(\"=== Error Handling During Data Loading ===\")\n",
    "\n",
    "# Create a file with some problematic data\n",
    "problematic_data = \"\"\"\n",
    "id,name,age,salary,date\n",
    "1,Alice,25,50000,2024-01-01\n",
    "2,Bob,invalid_age,60000,2024-01-02\n",
    "3,Charlie,30,not_a_number,2024-01-03\n",
    "4,Diana,35,70000,invalid_date\n",
    "5,Eve,28,55000,2024-01-05\n",
    "\"\"\"\n",
    "\n",
    "with open('problematic_data.csv', 'w') as f:\n",
    "    f.write(problematic_data)\n",
    "\n",
    "# Strategy 1: Load with error handling\n",
    "def load_with_error_handling() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data with comprehensive error handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # First, try to load with basic settings\n",
    "        df = pd.read_csv('problematic_data.csv')\n",
    "        print(f\"Basic load successful: {df.shape}\")\n",
    "        \n",
    "        # Handle data type conversions with error handling\n",
    "        df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "        df['salary'] = pd.to_numeric(df['salary'], errors='coerce')\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        \n",
    "        # Report data quality issues\n",
    "        print(f\"\\nData quality report:\")\n",
    "        for col in ['age', 'salary', 'date']:\n",
    "            null_count = df[col].isnull().sum()\n",
    "            if null_count > 0:\n",
    "                print(f\"  {col}: {null_count} invalid values converted to NaN\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "df_with_errors = load_with_error_handling()\n",
    "print(f\"\\nLoaded data with error handling:\")\n",
    "print(df_with_errors)\n",
    "print(f\"\\nData types after conversion:\")\n",
    "print(df_with_errors.dtypes)\n",
    "\n",
    "# Strategy 2: Skip bad lines\n",
    "print(f\"\\n=== Alternative: Skip Bad Lines ===\")\n",
    "try:\n",
    "    df_skip_bad = pd.read_csv(\n",
    "        'problematic_data.csv',\n",
    "        error_bad_lines=False,\n",
    "        warn_bad_lines=True\n",
    "    )\n",
    "    print(f\"Loaded with skip bad lines: {df_skip_bad.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Skip bad lines approach failed: {e}\")\n",
    "\n",
    "# Clean up temporary files\n",
    "import os\n",
    "temp_files = ['sales_data.csv', 'customers.json', 'products.tsv', 'large_sample.csv', 'problematic_data.csv']\n",
    "for file in temp_files:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "\n",
    "print(f\"\\n✓ Temporary files cleaned up\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}