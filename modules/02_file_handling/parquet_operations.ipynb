{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet File Operations\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will be able to:\n",
    "- Understand what Parquet files are and their advantages\n",
    "- Read and write Parquet files using pandas\n",
    "- Work with Parquet file metadata and schema\n",
    "- Handle large datasets efficiently with Parquet\n",
    "- Convert between different file formats (CSV, JSON, Parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Parquet Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# What is Parquet?\n",
    "print(\"Apache Parquet is a columnar storage file format that provides:\")\n",
    "print(\"✓ Efficient compression and encoding\")\n",
    "print(\"✓ Fast query performance\")\n",
    "print(\"✓ Schema evolution support\")\n",
    "print(\"✓ Cross-language compatibility\")\n",
    "print(\"✓ Optimized for analytics workloads\")\n",
    "print()\n",
    "print(\"Advantages over CSV:\")\n",
    "print(\"• Smaller file sizes (better compression)\")\n",
    "print(\"• Faster read/write operations\")\n",
    "print(\"• Preserves data types\")\n",
    "print(\"• Supports nested data structures\")\n",
    "print(\"• Built-in metadata and schema information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample sales data for demonstration\n",
    "def generate_sample_sales_data(num_records: int = 10000) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate sample sales data for testing.\n",
    "    \n",
    "    Args:\n",
    "        num_records: Number of records to generate\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with sample sales data\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducible results\n",
    "    \n",
    "    # Generate date range\n",
    "    start_date = datetime(2023, 1, 1)\n",
    "    dates = [start_date + timedelta(days=x) for x in range(365)]\n",
    "    \n",
    "    # Sample data\n",
    "    products = ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam', 'Headphones', 'Tablet', 'Phone']\n",
    "    categories = ['Electronics', 'Accessories', 'Computing', 'Mobile']\n",
    "    regions = ['North', 'South', 'East', 'West', 'Central']\n",
    "    \n",
    "    data = {\n",
    "        'transaction_id': [f'TXN{i:06d}' for i in range(1, num_records + 1)],\n",
    "        'date': np.random.choice(dates, num_records),\n",
    "        'product': np.random.choice(products, num_records),\n",
    "        'category': np.random.choice(categories, num_records),\n",
    "        'region': np.random.choice(regions, num_records),\n",
    "        'quantity': np.random.randint(1, 10, num_records),\n",
    "        'unit_price': np.round(np.random.uniform(10.0, 500.0, num_records), 2),\n",
    "        'discount_percent': np.round(np.random.uniform(0.0, 20.0, num_records), 1),\n",
    "        'customer_id': [f'CUST{np.random.randint(1, 1000):04d}' for _ in range(num_records)],\n",
    "        'is_online': np.random.choice([True, False], num_records)\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Calculate derived fields\n",
    "    df['subtotal'] = df['quantity'] * df['unit_price']\n",
    "    df['discount_amount'] = df['subtotal'] * (df['discount_percent'] / 100)\n",
    "    df['total_amount'] = df['subtotal'] - df['discount_amount']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate sample data\n",
    "sales_df = generate_sample_sales_data(10000)\n",
    "\n",
    "print(f\"Generated {len(sales_df):,} sales records\")\n",
    "print(f\"\\nDataFrame info:\")\n",
    "print(sales_df.info())\n",
    "print(f\"\\nFirst few records:\")\n",
    "print(sales_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Writing Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Parquet writing\n",
    "def write_parquet_file(df: pd.DataFrame, filename: str, compression: str = 'snappy') -> bool:\n",
    "    \"\"\"\n",
    "    Write DataFrame to Parquet file.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to write\n",
    "        filename: Output filename\n",
    "        compression: Compression algorithm ('snappy', 'gzip', 'brotli', 'lz4')\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_parquet(filename, compression=compression, index=False)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing Parquet file: {e}\")\n",
    "        return False\n",
    "\n",
    "# Write with different compression algorithms\n",
    "compression_types = ['snappy', 'gzip', 'brotli']\n",
    "\n",
    "for compression in compression_types:\n",
    "    filename = f'sales_data_{compression}.parquet'\n",
    "    success = write_parquet_file(sales_df, filename, compression)\n",
    "    \n",
    "    if success:\n",
    "        file_size = os.path.getsize(filename)\n",
    "        print(f\"{compression.upper()} compression: {filename} ({file_size:,} bytes)\")\n",
    "\n",
    "# Compare with CSV size\n",
    "csv_filename = 'sales_data.csv'\n",
    "sales_df.to_csv(csv_filename, index=False)\n",
    "csv_size = os.path.getsize(csv_filename)\n",
    "print(f\"\\nCSV file: {csv_filename} ({csv_size:,} bytes)\")\n",
    "\n",
    "# Calculate compression ratios\n",
    "print(\"\\nCompression ratios compared to CSV:\")\n",
    "for compression in compression_types:\n",
    "    filename = f'sales_data_{compression}.parquet'\n",
    "    if os.path.exists(filename):\n",
    "        parquet_size = os.path.getsize(filename)\n",
    "        ratio = (csv_size - parquet_size) / csv_size * 100\n",
    "        print(f\"{compression.upper()}: {ratio:.1f}% smaller than CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reading Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Parquet reading\n",
    "def read_parquet_file(filename: str, columns: Optional[List[str]] = None) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Read Parquet file with optional column selection.\n",
    "    \n",
    "    Args:\n",
    "        filename: Parquet file to read\n",
    "        columns: Specific columns to read (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_parquet(filename, columns=columns)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Parquet file: {e}\")\n",
    "        return None\n",
    "\n",
    "# Read the entire file\n",
    "loaded_df = read_parquet_file('sales_data_snappy.parquet')\n",
    "\n",
    "if loaded_df is not None:\n",
    "    print(f\"Loaded {len(loaded_df):,} records from Parquet file\")\n",
    "    print(f\"\\nDataFrame info:\")\n",
    "    print(loaded_df.info())\n",
    "    \n",
    "    # Verify data integrity\n",
    "    print(f\"\\nData integrity check:\")\n",
    "    print(f\"Original records: {len(sales_df):,}\")\n",
    "    print(f\"Loaded records: {len(loaded_df):,}\")\n",
    "    print(f\"Data matches: {sales_df.equals(loaded_df)}\")\n",
    "\n",
    "# Read only specific columns (column pruning)\n",
    "selected_columns = ['transaction_id', 'date', 'product', 'total_amount']\n",
    "partial_df = read_parquet_file('sales_data_snappy.parquet', columns=selected_columns)\n",
    "\n",
    "if partial_df is not None:\n",
    "    print(f\"\\nLoaded {len(partial_df.columns)} columns: {list(partial_df.columns)}\")\n",
    "    print(f\"Sample data:\")\n",
    "    print(partial_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Working with Parquet Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Read Parquet file metadata\n",
    "def get_parquet_metadata(filename: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get metadata information from Parquet file.\n",
    "    \n",
    "    Args:\n",
    "        filename: Parquet file to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with metadata information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parquet_file = pq.ParquetFile(filename)\n",
    "        metadata = parquet_file.metadata\n",
    "        schema = parquet_file.schema\n",
    "        \n",
    "        info = {\n",
    "            'num_rows': metadata.num_rows,\n",
    "            'num_columns': metadata.num_columns,\n",
    "            'num_row_groups': metadata.num_row_groups,\n",
    "            'file_size': os.path.getsize(filename),\n",
    "            'created_by': metadata.created_by,\n",
    "            'schema': str(schema),\n",
    "            'columns': []\n",
    "        }\n",
    "        \n",
    "        # Get column information\n",
    "        for i in range(metadata.num_columns):\n",
    "            col_meta = metadata.row_group(0).column(i)\n",
    "            col_info = {\n",
    "                'name': schema.names[i],\n",
    "                'type': str(schema.types[i]),\n",
    "                'total_byte_size': col_meta.total_byte_size,\n",
    "                'compression': str(col_meta.compression)\n",
    "            }\n",
    "            info['columns'].append(col_info)\n",
    "        \n",
    "        return info\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading metadata: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Analyze Parquet file metadata\n",
    "metadata = get_parquet_metadata('sales_data_snappy.parquet')\n",
    "\n",
    "if metadata:\n",
    "    print(\"Parquet File Metadata:\")\n",
    "    print(f\"Rows: {metadata['num_rows']:,}\")\n",
    "    print(f\"Columns: {metadata['num_columns']}\")\n",
    "    print(f\"Row Groups: {metadata['num_row_groups']}\")\n",
    "    print(f\"File Size: {metadata['file_size']:,} bytes\")\n",
    "    print(f\"Created By: {metadata['created_by']}\")\n",
    "    \n",
    "    print(\"\\nColumn Information:\")\n",
    "    for col in metadata['columns']:\n",
    "        print(f\"  {col['name']}: {col['type']} ({col['total_byte_size']:,} bytes, {col['compression']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Performance comparison between CSV and Parquet\n",
    "def compare_read_performance(csv_file: str, parquet_file: str, iterations: int = 3) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compare read performance between CSV and Parquet files.\n",
    "    \n",
    "    Args:\n",
    "        csv_file: CSV file path\n",
    "        parquet_file: Parquet file path\n",
    "        iterations: Number of iterations for timing\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with timing results\n",
    "    \"\"\"\n",
    "    results = {'csv_times': [], 'parquet_times': []}\n",
    "    \n",
    "    # Test CSV reading\n",
    "    for i in range(iterations):\n",
    "        start_time = time.time()\n",
    "        csv_df = pd.read_csv(csv_file)\n",
    "        csv_time = time.time() - start_time\n",
    "        results['csv_times'].append(csv_time)\n",
    "        print(f\"CSV read {i+1}: {csv_time:.3f} seconds\")\n",
    "    \n",
    "    # Test Parquet reading\n",
    "    for i in range(iterations):\n",
    "        start_time = time.time()\n",
    "        parquet_df = pd.read_parquet(parquet_file)\n",
    "        parquet_time = time.time() - start_time\n",
    "        results['parquet_times'].append(parquet_time)\n",
    "        print(f\"Parquet read {i+1}: {parquet_time:.3f} seconds\")\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_csv = sum(results['csv_times']) / len(results['csv_times'])\n",
    "    avg_parquet = sum(results['parquet_times']) / len(results['parquet_times'])\n",
    "    \n",
    "    results['avg_csv'] = avg_csv\n",
    "    results['avg_parquet'] = avg_parquet\n",
    "    results['speedup'] = avg_csv / avg_parquet\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run performance comparison\n",
    "print(\"Performance Comparison: CSV vs Parquet\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "perf_results = compare_read_performance('sales_data.csv', 'sales_data_snappy.parquet')\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Average CSV read time: {perf_results['avg_csv']:.3f} seconds\")\n",
    "print(f\"Average Parquet read time: {perf_results['avg_parquet']:.3f} seconds\")\n",
    "print(f\"Parquet is {perf_results['speedup']:.1f}x faster than CSV\")\n",
    "\n",
    "# File size comparison\n",
    "csv_size = os.path.getsize('sales_data.csv')\n",
    "parquet_size = os.path.getsize('sales_data_snappy.parquet')\n",
    "size_reduction = (csv_size - parquet_size) / csv_size * 100\n",
    "\n",
    "print(f\"\\nFile Size Comparison:\")\n",
    "print(f\"CSV file: {csv_size:,} bytes\")\n",
    "print(f\"Parquet file: {parquet_size:,} bytes\")\n",
    "print(f\"Size reduction: {size_reduction:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. File Format Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert between different file formats\n",
    "def convert_csv_to_parquet(csv_file: str, parquet_file: str, chunk_size: int = 10000) -> bool:\n",
    "    \"\"\"\n",
    "    Convert CSV file to Parquet format in chunks for memory efficiency.\n",
    "    \n",
    "    Args:\n",
    "        csv_file: Input CSV file\n",
    "        parquet_file: Output Parquet file\n",
    "        chunk_size: Number of rows to process at once\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read CSV in chunks and write to Parquet\n",
    "        first_chunk = True\n",
    "        \n",
    "        for chunk_df in pd.read_csv(csv_file, chunksize=chunk_size):\n",
    "            if first_chunk:\n",
    "                # Write first chunk (creates file)\n",
    "                chunk_df.to_parquet(parquet_file, index=False)\n",
    "                first_chunk = False\n",
    "            else:\n",
    "                # Append subsequent chunks\n",
    "                existing_df = pd.read_parquet(parquet_file)\n",
    "                combined_df = pd.concat([existing_df, chunk_df], ignore_index=True)\n",
    "                combined_df.to_parquet(parquet_file, index=False)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error converting CSV to Parquet: {e}\")\n",
    "        return False\n",
    "\n",
    "def convert_json_to_parquet(json_file: str, parquet_file: str) -> bool:\n",
    "    \"\"\"\n",
    "    Convert JSON file to Parquet format.\n",
    "    \n",
    "    Args:\n",
    "        json_file: Input JSON file\n",
    "        parquet_file: Output Parquet file\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read JSON and convert to Parquet\n",
    "        df = pd.read_json(json_file)\n",
    "        df.to_parquet(parquet_file, index=False)\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error converting JSON to Parquet: {e}\")\n",
    "        return False\n",
    "\n",
    "def convert_parquet_to_csv(parquet_file: str, csv_file: str) -> bool:\n",
    "    \"\"\"\n",
    "    Convert Parquet file to CSV format.\n",
    "    \n",
    "    Args:\n",
    "        parquet_file: Input Parquet file\n",
    "        csv_file: Output CSV file\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read Parquet and convert to CSV\n",
    "        df = pd.read_parquet(parquet_file)\n",
    "        df.to_csv(csv_file, index=False)\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error converting Parquet to CSV: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create sample JSON data for conversion testing\n",
    "sample_json_data = [\n",
    "    {\"id\": 1, \"name\": \"Product A\", \"price\": 29.99, \"category\": \"Electronics\"},\n",
    "    {\"id\": 2, \"name\": \"Product B\", \"price\": 49.99, \"category\": \"Accessories\"},\n",
    "    {\"id\": 3, \"name\": \"Product C\", \"price\": 19.99, \"category\": \"Electronics\"}\n",
    "]\n",
    "\n",
    "# Write sample JSON file\n",
    "import json\n",
    "with open('sample_products.json', 'w') as f:\n",
    "    json.dump(sample_json_data, f, indent=2)\n",
    "\n",
    "# Test conversions\n",
    "print(\"File Format Conversions:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# JSON to Parquet\n",
    "if convert_json_to_parquet('sample_products.json', 'products.parquet'):\n",
    "    print(\"✓ JSON to Parquet conversion successful\")\n",
    "    \n",
    "    # Verify the conversion\n",
    "    products_df = pd.read_parquet('products.parquet')\n",
    "    print(f\"  Converted {len(products_df)} products\")\n",
    "    print(f\"  Columns: {list(products_df.columns)}\")\n",
    "\n",
    "# Parquet to CSV\n",
    "if convert_parquet_to_csv('products.parquet', 'products_from_parquet.csv'):\n",
    "    print(\"✓ Parquet to CSV conversion successful\")\n",
    "    \n",
    "    # Show file sizes\n",
    "    json_size = os.path.getsize('sample_products.json')\n",
    "    parquet_size = os.path.getsize('products.parquet')\n",
    "    csv_size = os.path.getsize('products_from_parquet.csv')\n",
    "    \n",
    "    print(f\"\\nFile sizes:\")\n",
    "    print(f\"  JSON: {json_size} bytes\")\n",
    "    print(f\"  Parquet: {parquet_size} bytes\")\n",
    "    print(f\"  CSV: {csv_size} bytes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}