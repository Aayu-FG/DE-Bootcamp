{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Load vs Incremental Load Case Study\n",
    "\n",
    "## Learning Goals\n",
    "- Understand Full Load vs Incremental Load concepts\n",
    "- Work with CSV and JSON files using pandas\n",
    "- Merge data from different sources\n",
    "- Apply basic data transformations\n",
    "\n",
    "## Key Concepts\n",
    "- **Full Load**: Process all data from scratch every time\n",
    "- **Incremental Load**: Process only new data since last run\n",
    "\n",
    "## Scenario\n",
    "You work for a retail company that gets daily sales data in different formats. You need to combine and process this data efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: Create Sample Data Files\n",
    "\n",
    "First, let's create sample data files with the same schema in both CSV and JSON formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Full Load Data (CSV) - Complete Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sales_full.csv\n",
    "transaction_id,customer_id,product_name,quantity,price,sale_date\n",
    "TXN001,CUST001,Laptop,1,999.99,2024-01-01\n",
    "TXN002,CUST002,Mouse,2,25.50,2024-01-01\n",
    "TXN003,CUST001,Keyboard,1,75.00,2024-01-02\n",
    "TXN004,CUST003,Monitor,1,299.99,2024-01-02\n",
    "TXN005,CUST002,Headphones,1,89.99,2024-01-03\n",
    "TXN006,CUST003,Webcam,1,49.99,2024-01-03\n",
    "TXN007,CUST001,Tablet,1,399.99,2024-01-04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Incremental Load Data (JSON) - New Transactions Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sales_new.json\n",
    "[\n",
    "  {\n",
    "    \"transaction_id\": \"TXN008\",\n",
    "    \"customer_id\": \"CUST002\",\n",
    "    \"product_name\": \"Speaker\",\n",
    "    \"quantity\": 1,\n",
    "    \"price\": 79.99,\n",
    "    \"sale_date\": \"2024-01-05\"\n",
    "  },\n",
    "  {\n",
    "    \"transaction_id\": \"TXN009\",\n",
    "    \"customer_id\": \"CUST004\",\n",
    "    \"product_name\": \"Phone\",\n",
    "    \"quantity\": 1,\n",
    "    \"price\": 699.99,\n",
    "    \"sale_date\": \"2024-01-05\"\n",
    "  },\n",
    "  {\n",
    "    \"transaction_id\": \"TXN010\",\n",
    "    \"customer_id\": \"CUST003\",\n",
    "    \"product_name\": \"Cable\",\n",
    "    \"quantity\": 3,\n",
    "    \"price\": 15.99,\n",
    "    \"sale_date\": \"2024-01-06\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2: Load Data with Pandas\n",
    "\n",
    "Now let's load both files using pandas and understand the difference between full load and incremental load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL LOAD: Load complete dataset from CSV\n",
    "print(\"=== FULL LOAD ===\")\n",
    "print(\"Loading complete dataset from CSV file...\")\n",
    "\n",
    "df_full = pd.read_csv('sales_full.csv')\n",
    "print(f\"Full load dataset shape: {df_full.shape}\")\n",
    "print(\"\\nFull dataset:\")\n",
    "print(df_full)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# INCREMENTAL LOAD: Load only new data from JSON\n",
    "print(\"=== INCREMENTAL LOAD ===\")\n",
    "print(\"Loading only new transactions from JSON file...\")\n",
    "\n",
    "df_new = pd.read_json('sales_new.json')\n",
    "print(f\"Incremental load dataset shape: {df_new.shape}\")\n",
    "print(\"\\nNew transactions:\")\n",
    "print(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3: Merge Data from Both Sources\n",
    "\n",
    "Let's combine the full load data with the incremental load data to create a complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge full load and incremental load data\n",
    "print(\"Merging full load and incremental load data...\")\n",
    "\n",
    "# Combine both dataframes\n",
    "df_combined = pd.concat([df_full, df_new], ignore_index=True)\n",
    "\n",
    "print(f\"Full load records: {len(df_full)}\")\n",
    "print(f\"Incremental load records: {len(df_new)}\")\n",
    "print(f\"Combined dataset records: {len(df_combined)}\")\n",
    "\n",
    "print(\"\\nCombined dataset:\")\n",
    "print(df_combined)\n",
    "\n",
    "# Check data types and basic info\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df_combined.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 4: Remove Duplicates and Apply Transformations\n",
    "\n",
    "Let's clean the data and apply some basic transformations and aggregations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Check for and Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate transactions\n",
    "print(\"Checking for duplicates...\")\n",
    "print(f\"Total records before deduplication: {len(df_combined)}\")\n",
    "\n",
    "# Check if there are any duplicate transaction_ids\n",
    "duplicates = df_combined[df_combined.duplicated(subset=['transaction_id'], keep=False)]\n",
    "print(f\"Duplicate transactions found: {len(duplicates)}\")\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    print(\"\\nDuplicate records:\")\n",
    "    print(duplicates)\n",
    "\n",
    "# Remove duplicates based on transaction_id\n",
    "df_clean = df_combined.drop_duplicates(subset=['transaction_id'], keep='first')\n",
    "print(f\"\\nRecords after deduplication: {len(df_clean)}\")\n",
    "print(f\"Removed {len(df_combined) - len(df_clean)} duplicate records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Add Calculated Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add calculated columns\n",
    "print(\"Adding calculated columns...\")\n",
    "\n",
    "# Calculate total amount for each transaction\n",
    "df_clean['total_amount'] = df_clean['quantity'] * df_clean['price']\n",
    "\n",
    "# Convert sale_date to datetime\n",
    "df_clean['sale_date'] = pd.to_datetime(df_clean['sale_date'])\n",
    "\n",
    "# Add month column\n",
    "df_clean['sale_month'] = df_clean['sale_date'].dt.strftime('%Y-%m')\n",
    "\n",
    "print(\"\\nDataset with calculated columns:\")\n",
    "print(df_clean[['transaction_id', 'product_name', 'quantity', 'price', 'total_amount', 'sale_month']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3: Apply Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by customer and calculate totals\n",
    "print(\"=== CUSTOMER ANALYSIS ===\")\n",
    "customer_summary = df_clean.groupby('customer_id').agg({\n",
    "    'total_amount': ['sum', 'count', 'mean'],\n",
    "    'quantity': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "customer_summary.columns = ['total_spent', 'num_transactions', 'avg_transaction', 'total_items']\n",
    "customer_summary = customer_summary.reset_index()\n",
    "\n",
    "print(\"Customer Summary:\")\n",
    "print(customer_summary)\n",
    "\n",
    "print(\"\\n=== PRODUCT ANALYSIS ===\")\n",
    "product_summary = df_clean.groupby('product_name').agg({\n",
    "    'quantity': 'sum',\n",
    "    'total_amount': 'sum',\n",
    "    'transaction_id': 'count'\n",
    "}).round(2)\n",
    "\n",
    "product_summary.columns = ['total_quantity_sold', 'total_revenue', 'num_sales']\n",
    "product_summary = product_summary.reset_index().sort_values('total_revenue', ascending=False)\n",
    "\n",
    "print(\"Product Summary (sorted by revenue):\")\n",
    "print(product_summary)\n",
    "\n",
    "print(\"\\n=== OVERALL SUMMARY ===\")\n",
    "print(f\"Total Revenue: ${df_clean['total_amount'].sum():.2f}\")\n",
    "print(f\"Total Transactions: {len(df_clean)}\")\n",
    "print(f\"Average Transaction Value: ${df_clean['total_amount'].mean():.2f}\")\n",
    "print(f\"Total Items Sold: {df_clean['quantity'].sum()}\")\n",
    "print(f\"Unique Customers: {df_clean['customer_id'].nunique()}\")\n",
    "print(f\"Unique Products: {df_clean['product_name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "**Full Load vs Incremental Load:**\n",
    "- **Full Load**: Process complete dataset from scratch (like our CSV file)\n",
    "- **Incremental Load**: Process only new data since last run (like our JSON file)\n",
    "\n",
    "**Key Skills:**\n",
    "- Loading CSV and JSON files with pandas\n",
    "- Merging data from different sources with same schema\n",
    "- Removing duplicates to ensure data quality\n",
    "- Adding calculated columns and transformations\n",
    "- Creating aggregations and summaries\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Daily sales data processing\n",
    "- Customer analytics\n",
    "- Product performance analysis\n",
    "- Data quality management\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Full Load** is good for initial loads or when you need to reprocess everything\n",
    "2. **Incremental Load** is more efficient for regular updates with only new data\n",
    "3. **Same schema** makes it easy to merge data from different file formats\n",
    "4. **Data cleaning** (removing duplicates) is essential for accurate analysis\n",
    "5. **Pandas** makes data processing much easier than basic Python file operations\n",
    "\n",
    "Great job completing this case study! You now understand fundamental data loading patterns used in real data engineering projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}